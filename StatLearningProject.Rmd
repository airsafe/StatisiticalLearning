---
title: "Practical Machine Learning Project"
author: "Todd Curtis"
date: "April 22, 2015"
output: html_document
---
####Background and Executive Summary
Devices that measure detailed, quantified, information about personal activity allow the measurement of both the quantity and quality of that activity performed by the four study subjects. The data in this project comes from accelerometers placed on the belt, forearm, arm, and dumbell of six participants. They were asked to perform one set of 10 repetitions of barbell lifts correctly, and also incorrectly in four different ways. The correct method was designated class A, and the four incorrect methods were designated class B, C, D, and E.

These five different kinds of outcomes were associated with five distinct output classifications, and were coded as the dependent variable in the training data set.

After evaluating a Random Forest and Classification and Regression Tree (CART) prediction model, the Random Forest model was chosen due to its higher accuracy when tested on the training set.

Based on an evaluation of the accuracy of the Random Forest model using a k-fold cross-validation (with k=10), the estimated out of sample error rate was 0.0035, with a 95% confidence interval of (0.0029, 0.0041).

This model was used on the 20 cases from the test set, with seven of the 20 cases predicted to be in the class associated with the correct barbell lifting method (class A), and at least one case associated with each of the other four classes.

####Project
The goal of the project was to take two sets of data, one a training set and one a testing set, and predict the dependent variable values, specifically one of five distinct classes, associated with the test data.

The major stages of the project including taking training data, using that data to choose an appropriate prediction model, esitmate the out of sample error associated with that model, and then to apply that algorithm to a set of test data.

The data for this project come from  source: http://groupware.les.inf.puc-rio.br/har, and is contained in two files:
- pml-training.csv (19622 rows)
- pml-testing.csv (20 rows)

##### Project Objectives
- Create a report describing how the prediction model was created, 
- Description of how cross validation was used, 
- Create an expected value of the out of sample error (1-accuracy),
- Explain why the chocies in the analysis wer made, and
- Use the prediction model that was created to predict the class associated with 20 test cases. 

####Data
Both sets of data consists of 160 variables. The dependent variable in the training set consists of the values of the five class values associated with the exercise outcomes of the study participants. variables in the training set has 19,622 rows of observations, representing the data from the combinations of study subjects and exercise activities. 

The test set consists of 20 rows of data with independent variables that were identical to those of the training set. Instead of a dependent variable being the exercise classification, the test set had a variable the served as an identifier of the sample.

**Eliminating variables**

A review of the training data revealed that for 100 of the 159 independent variables in the training set had missing or NA data values for roughly 98% of the rows. Those variables (columns) were eliminated from the analyses associated with choosing and evaluating the model.

Also eliminated were seven variables (columns 1-7 of the training and data set), which appeared to be unrelated to the execution of the exercices by the six subjects. 

After eliminating largely empty or irrelevant variables, both the test and training sets consisted of data from 52 independent variables. In addition, there was one dependent variable in the training set, along with an identification vector in the test set.

```{r, echo=TRUE}
# url_raw_training <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
# download.file(url=url_raw_training, destfile="file_dest_training", method="curl")

train.raw = read.csv("pml-training.csv")
plot(train.raw$classe, main="Distirbution of classes in the training set", xlab="Class", ylab="Number of outcomes", col="blue")

test.raw = read.csv("pml-testing.csv")

# Eliminated the first seven columns from test and training data because they were not relevant to the prediction.
train.raw = train.raw[,8:ncol(train.raw)]
test.raw = test.raw[,8:ncol(test.raw)]

# Counting presence of NA or missing values in training set
paste("A total of ",sum(apply(train.raw,2,function(x) (anyNA(x)) | any(x=="") ))," variables have at least one NA value or one blank value.",sep="")
# 100 variables have at least one NA or blank variable


elim = apply(train.raw,2,function(x) (anyNA(x)) | any(x=="") ) # Vector of largely NA variables
# summary(train.raw[,elim])
# A review of the summary of these 100 variable revealled that at least 19,216 out of 19,622 (about 98% of the values) were either blank or NA, and where therefore to be eliminated from further consideration
sum(!elim)
train = train.raw[,elim==FALSE] # Eliminated variables with NA and blank values
test = test.raw[,elim==FALSE] # Eliminated variables with NA and blank values
str(train)
str(test)
```
####Choosing an appropriate model
Because the independent variable consists of a five-level factor variable, two candidate prediction models, Random Forest and Classification and Regression Tree (CART), were evaluated to see which one had a higher level of accuracy  on the training set. 

The model with the highest accuracy (which would also have the lowest out of sample error rate), would be chosen to predict the exercise categories associated with the testing set. 

The analysis below shows that the Random Forest model had an accuracy of 0.997, which was significantly higher than the CART model's accuracy, which was 0.756. This was the prediction model used in the remainder of this analysis.


```{r, echo=FALSE}

# The package caTools has basic utility functions plus things like calculation of AUC
# install.packages("caTools", repos="http://cran.rstudio.com/") 
library(caTools)

# install.packages("caret", repos="http://cran.rstudio.com/") 
# Install cross-validation packages
library(caret)
# install.packages("e1071")
library(e1071)

# For CART modeling
# install.packages("rpart")
library(rpart)
# install.packages("rpart.plot")
library(rpart.plot)

# Install randomForest package
# install.packages("randomForest")
library(randomForest)
```


```{r, echo=TRUE}
# Random forest model using the entire training set
forest.model = randomForest(classe ~ ., data=train, method="class")
predict.forest.model = predict(forest.model, data = train, type="class")
confusionMatrix(predict.forest.model, train$classe)

# Evaluate the performance of the Random Forest model
forest.accuracy = sum(diag(table(predict.forest.model, train$classe)))/nrow(train)
paste("Accuracy of Random Forest model is",format(forest.accuracy,digits=4) )

# CART classification tree model using the entire training set
cart.model = rpart(classe ~ ., data=train, method="class") 
prp(cart.model) # display the tree model

# Evaluate the performance of the CART model
predict.cart.model = predict(cart.model, data=train, type="class")
cart.accuracy = sum(diag(table(train$classe, predict.cart.model)))/nrow(train)
paste("Accuracy of CART model is",format(cart.accuracy,digits=3) )

print("Predicted classes using the CART model")
table(predict.cart.model)

print("Predicted classes using the Random Forest model")
table(predict.forest.model)

print("Actual classes")
table(train$classe)

```

####Estimating out of sample error  with k-fold cross-validation
The out of sample error was estimated using a k-fold cross-validation on the training set. In the case of a k-fold validation process, the rows of the training data were first randomized, and then split into k roughly equal sized subsets or folds, each containing (k-1) subsets as the training data and the kth subset on the training data. The model is run k times, where for each run of the model, a different one of the k subsets is used as the test data. The training set had 19,622 rows of data, and k=10 folds were used to validate the outs of sample error rate.

The size of each fold is n = floor(nrow(training_set)/k). So for k=10, k-1 folds (about 90% of data) represented the training set, and the testing set was about 10% of the total data (roughly 1,962 rows of data).

The advantage of this method is that it matters less how the data gets divided. Every data point gets to be in a test set exactly once, and gets to be in a training set k-1 times. The variance of the resulting estimate is reduced as k is increased. 

**Estimate of expected out of sample error**

The out of sample error is equal to (1-accuracy). After the k-fold validation of the accuracy of the Random Forest prediction of the dependent variable values, the mean value of accuracy value was 0.9965, and the standard deviation was 0.0003203, giving a 95% confidence interval of (0.9959, 0.9971). The estimated out of sample error was therefore 0.0035, with a 95% confidence interval of (0.0029, 0.0041).

```{r, echo=TRUE}

# Training set shuffled and randomized
set.seed(29)
train.ran <- train[sample(nrow(train)),]

# For a k-fold randomization 
k = 10
n = floor(nrow(train.ran)/k) # n=1962 is the size of first nine folds, with the 10th fold of size nrow(train.ran)%%n (n+ remainder of nrow(train)/k*n)
acc.vec <- vector("numeric", length = k)
fold=data.frame("start"=numeric(), "finish"=numeric())
for(i in 1:k){
  fold[i,1] = ((i-1)*n + 1) #the start of the subset
  fold[i,2] = (i*n)       #the end of the subset
  if(i==k){
          fold[i,2] = fold[i,2] + nrow(train.ran)%%n 
          # Adds remainder of nrow(train)/n to get last fold size
  }
  range = fold[i,1]:fold[i,2] 
  cv.train = train.ran[-range,] 
  # In each fold, all but last training set is size (k-1)*n + nrow(train.ran)%%n 
  # The kth training set is of size (k-1)*n 
  cv.test = train.ran[range,] #test the model's performance on this data
  forest.model.cv = randomForest(classe ~ ., data=cv.train)
  predict.forest.model.cv = predict(forest.model.cv, data=cv.train, type="response")

  # Evaluate the accuracy of the Random Forest model
  forest.accuracy.cv = sum(diag(table(predict.forest.model.cv, cv.train$classe)))/nrow(cv.train)
  acc.vec[i] = forest.accuracy.cv
print(paste("Accuracy of Random Forest model run ",i, " is ",format(forest.accuracy.cv,digits=4),"." ,sep=""))
}
paste("Mean accuracy is ",format(mean(acc.vec), digits=4)," standard deviation is ",format(sd(acc.vec), digits=4), sep="")
print("Fold boundaries")
fold
paste("Testing set size in first ", k-1," folds were of size ",n,", and the last was of size ",(n + nrow(train)%%n),"." ,sep="")
```

####Predicted test set classifications
The test set was considerably smaller, consisting of only 20 rows of data. Based on this data, the Random Forest prediction algorithm that was created early predicted that the 20 observations would be distributed among the five classes as follows:

- A: 7 (Cases 2,4,5,9, 10, 14, and 17)
- B: 8 (Cases 1,3,8,11,13,18,19, and 20)
- C: 1 (Case 12)
- D: 1 (Case 7)
- E: 3 (Cases 6, 15, and 16)


```{r, echo=TRUE}

predict.forest.model.test = predict(forest.model, newdata = test, type="response")
table(predict.forest.model.test)
predict.forest.model.test
```
